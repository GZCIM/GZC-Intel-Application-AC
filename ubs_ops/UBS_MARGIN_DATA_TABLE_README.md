# UBS Margin Data Database Structure

## Overview
This database structure is designed to store daily UBS margin CSV files with filename tracking to prevent duplicate processing when files are regenerated by the scheduler.

## Table Structure

### Main Table: `ubs_margin_data`
Stores all individual records from the CSV files.

**Key Features:**
- **`source_filename`**: Tracks the original filename to prevent overwrites
- **`record_hash`**: Unique hash based on key fields for duplicate detection
- **`created_at`**: Timestamp when record was inserted
- All 27 columns from the UBS CSV file

**Primary Key:** `id` (auto-incrementing)

**Unique Constraint:** `record_hash` (prevents duplicate records)

### Summary Table: `ubs_margin_summary_daily`
Pre-calculated daily margin summaries for faster reporting.

**Key Features:**
- Stores calculated totals per account/date/filename
- Includes all margin summary components
- Automatically updated via function

**Unique Constraint:** `(account, cob_date, source_filename)`

### Processing Log: `ubs_file_processing_log`
Tracks file processing history and status.

**Key Features:**
- Prevents duplicate processing
- Tracks processing status (pending, processing, completed, failed)
- Records error messages for failed files
- Tracks file size and record count

## Installation

### 1. Create Tables
```bash
psql -U your_user -d your_database -f create_ubs_margin_data_table.sql
```

Or using the MCP Postgres server:
```python
# Read and execute the SQL file
```

### 2. Install Python Dependencies
```bash
pip install psycopg2-binary
```

## Usage

### Loading CSV Files

#### Basic Usage
```bash
python load_ubs_csv_to_database.py c:\tmp\20251110.MFXCMDRCSV.I0004255.CSV
```

#### With Custom Connection String
```bash
python load_ubs_csv_to_database.py file.csv "postgresql://user:pass@host:port/db"
```

#### Using Environment Variable
```bash
export POSTGRES_CONNECTION_STRING="postgresql://user:pass@host:port/db"
python load_ubs_csv_to_database.py file.csv
```

### Features

1. **Duplicate Prevention**
   - Checks `record_hash` before inserting
   - Skips records that already exist
   - Prevents duplicate processing of same file

2. **File Tracking**
   - Logs all processed files in `ubs_file_processing_log`
   - Tracks processing status and errors
   - Prevents reprocessing completed files

3. **Automatic Summary Calculation**
   - Calculates daily margin summaries automatically
   - Stores in `ubs_margin_summary_daily` table
   - Can be queried for fast reporting

## Query Examples

### Get Latest Data for an Account
```sql
SELECT * 
FROM ubs_margin_data 
WHERE account = 'I0004255' 
  AND cob_date = '2025-11-10'
ORDER BY created_at DESC;
```

### Get Daily Summary
```sql
SELECT 
    account,
    cob_date,
    total_market_value,
    total_margin,
    excess,
    otc_mtm_mv,
    money_market_mv,
    net_cash_mv
FROM ubs_margin_summary_daily
WHERE account = 'I0004255'
  AND cob_date >= '2025-11-01'
ORDER BY cob_date DESC;
```

### Check File Processing Status
```sql
SELECT 
    filename,
    account,
    cob_date,
    processing_status,
    record_count,
    started_at,
    completed_at,
    error_message
FROM ubs_file_processing_log
WHERE cob_date >= CURRENT_DATE - INTERVAL '7 days'
ORDER BY started_at DESC;
```

### Find All Files for an Account
```sql
SELECT DISTINCT 
    source_filename,
    cob_date,
    COUNT(*) as record_count,
    MAX(created_at) as last_updated
FROM ubs_margin_data
WHERE account = 'I0004255'
GROUP BY source_filename, cob_date
ORDER BY cob_date DESC;
```

### Get Latest Data Per Account/Date (Using View)
```sql
SELECT * 
FROM v_ubs_margin_data_latest
WHERE account = 'I0004255'
  AND cob_date = '2025-11-10';
```

## Scheduled Processing

### Example: Process Daily Files Automatically

Create a scheduled task (cron job on Linux or Task Scheduler on Windows):

```bash
# Daily at 2 AM
0 2 * * * /usr/bin/python3 /path/to/load_ubs_csv_to_database.py /path/to/daily/file.csv
```

### Example: Process Multiple Files
```python
import os
import glob
from load_ubs_csv_to_database import load_csv_to_database

db_conn = os.getenv('POSTGRES_CONNECTION_STRING')
csv_dir = r'c:\tmp'

# Process all CSV files matching pattern
for csv_file in glob.glob(os.path.join(csv_dir, '*.MFXCMDRCSV.*.CSV')):
    print(f"Processing {csv_file}...")
    load_csv_to_database(csv_file, db_conn)
```

## Data Types

| CSV Column | Database Column | Type | Notes |
|------------|----------------|------|-------|
| Account | account | VARCHAR(50) | NOT NULL |
| COB_Date | cob_date | DATE | NOT NULL |
| FX-Rate | fx_rate | DECIMAL(20,8) | Note: hyphen removed |
| MV_Rollup | mv_rollup | DECIMAL(20,2) | |
| Margin_Rollup | margin_rollup | DECIMAL(20,2) | |
| Quantity | quantity | DECIMAL(20,8) | |
| All text fields | Various | VARCHAR | Lengths vary by field |

## Indexes

The following indexes are created for performance:

- `idx_ubs_margin_account` - On account
- `idx_ubs_margin_cob_date` - On cob_date
- `idx_ubs_margin_filename` - On source_filename
- `idx_ubs_margin_account_date` - Composite on (account, cob_date)
- `idx_ubs_margin_margin_type` - On margin_type
- `idx_ubs_margin_product` - On product
- `idx_ubs_margin_created_at` - On created_at
- `idx_ubs_margin_file_processed_date` - On file_processed_date
- `idx_ubs_margin_account_date_type` - Composite on (account, cob_date, margin_type)

## Maintenance

### Check for Duplicate Files
```sql
SELECT 
    source_filename,
    COUNT(DISTINCT cob_date) as date_count,
    COUNT(*) as total_records
FROM ubs_margin_data
GROUP BY source_filename
HAVING COUNT(DISTINCT cob_date) > 1;
```

### Clean Up Old Data (if needed)
```sql
-- Delete data older than 1 year (use with caution)
DELETE FROM ubs_margin_data 
WHERE cob_date < CURRENT_DATE - INTERVAL '1 year';

-- Delete old processing logs
DELETE FROM ubs_file_processing_log 
WHERE completed_at < CURRENT_TIMESTAMP - INTERVAL '90 days';
```

### Vacuum and Analyze
```sql
VACUUM ANALYZE ubs_margin_data;
VACUUM ANALYZE ubs_margin_summary_daily;
VACUUM ANALYZE ubs_file_processing_log;
```

## Error Handling

The script handles:
- Duplicate records (skips them)
- Already processed files (asks for confirmation)
- Database connection errors
- CSV parsing errors
- Transaction rollback on errors

All errors are logged in `ubs_file_processing_log` table.

## Security Considerations

1. **Connection String**: Store securely, use environment variables
2. **File Permissions**: Ensure CSV files are readable
3. **Database Permissions**: Grant appropriate permissions to application user
4. **Backup**: Regular backups recommended for production

## Performance Tips

1. **Batch Processing**: Process multiple files in a single transaction
2. **Index Usage**: Queries use indexes automatically
3. **Summary Table**: Use `ubs_margin_summary_daily` for reporting instead of calculating on-the-fly
4. **Partitioning**: Consider table partitioning by date for very large datasets

## Support

For issues or questions:
1. Check `ubs_file_processing_log` for error messages
2. Verify database connection string
3. Ensure CSV file format matches expected structure
4. Check database permissions

